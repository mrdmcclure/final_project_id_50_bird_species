{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 883439,
          "sourceType": "datasetVersion",
          "datasetId": 471346
        }
      ],
      "dockerImageVersionId": 30775,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\"\"\"\n",
        "Premise:\n",
        "I was looking for a better way to bring in new data and new bird species to the 200 bird species Phase 2 project then what I had done earlier\n",
        "Earlier the data lived on a Google drive, where adding to it meant manual uploading and cleaning was either manual or done by running functions that looped through each folder\n",
        "\n",
        "Assumptions:\n",
        "That there can be a pipeline that I can create or modify which can bring in new data from an online source, ie Kaggle or the Cornel Lab\n",
        "\n",
        "Intital findings:\n",
        "Internet research turned up this Kaggle submission: https://www.kaggle.com/code/ikkiocean/bird-species-classification-using-dl\n",
        "I wanted to try it first\n",
        "\n",
        "IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "THEN FEEL FREE TO DELETE THIS CELL.\n",
        "\n",
        "NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR NOTEBOOK.\n",
        "Below copied from Kaggle:\n",
        "\n",
        "\"\"\"\n",
        "#import kagglehub\n",
        "#veeralakrishna_200_bird_species_with_11788_images_path = kagglehub.dataset_download('veeralakrishna/200-bird-species-with-11788-images')\n",
        "\n",
        "#by default, the Kaggle API downloads to my computer's catche.  I want it to go to Google drive instead\n",
        "\n",
        "#print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lo4Qm0rSHA7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "f8f621cc-91d4-4483-c023-2f667d01e5e4"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPremise:\\nI was looking for a better way to bring in new data and new bird species to the 200 bird species Phase 2 project then what I had done earlier\\nEarlier the data lived on a Google drive, where adding to it meant manual uploading and cleaning was either manual or done by running functions that looped through each folder\\n\\nAssumptions:\\nThat there can be a pipeline that I can create or modify which can bring in new data from an online source, ie Kaggle or the Cornel Lab\\n\\nIntital findings:\\nInternet research turned up this Kaggle submission: https://www.kaggle.com/code/ikkiocean/bird-species-classification-using-dl\\nI wanted to try it first\\n\\nIMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\\nTHEN FEEL FREE TO DELETE THIS CELL.\\n \\nNOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR NOTEBOOK.\\nBelow copied from Kaggle:\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# to import to google drive, first we need to mount it\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#this will prompt a pop up to log in and authinticate Google Drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xULvS1g7MbOa",
        "outputId": "ec1eba0f-fd9b-47d3-9731-adb55ebbdedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#next we need to import the data from Kaggle using Kagglehub\n",
        "\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "model_path = 'veeralakrishna/200-bird-species-with-11788-images'\n",
        "\n",
        "# Replace model_path with the actual model or dataset path from KaggleHub\n",
        "# For example, 'kagglehub/yolov8' for the YOLOv8 model\n",
        "# Or 'kagglehub/mnist' for a dataset\n",
        "#model_path = 'kagglehub/yolov8'\n",
        "\n",
        "\n",
        "# Specify the destination folder in your Google Drive\n",
        "destination_path = '/content/drive/My Drive/KaggleData'\n",
        "\n",
        "# Download the data\n",
        "downloaded_path = kagglehub.dataset_download(model_path, force_download=False)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Downloaded content is at: {downloaded_path}\")\n",
        "\n",
        "# Now, copy the downloaded content to your Google Drive\n",
        "# Make sure to create the destination folder if it doesn't exist, hance the code below\n",
        "!mkdir -p \"$destination_path\"\n",
        "!cp -r \"$downloaded_path\" \"$destination_path\"\n",
        "\n",
        "print(f\"Content copied to Google Drive at: {destination_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAtb5ByGM7QL",
        "outputId": "5fd40194-b171-4759-be22-551e8fa41f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the '200-bird-species-with-11788-images' dataset.\n",
            "Downloaded content is at: /kaggle/input/200-bird-species-with-11788-images\n",
            "Content copied to Google Drive at: /content/drive/My Drive/KaggleData\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the above imported .tnz zipped files to Google drive that need to be unzipped\n",
        "\n"
      ],
      "metadata": {
        "id": "eRiNQiLgM7bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xr2LJSxEM7oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need the file names\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd # I have a habit of adding these to python notebooks :D\n",
        "\n",
        "# According to the Kaggle submission, input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os  # will need this to loop through files, similiar to how the cleaning functions created earlier worked\n",
        "for dirname, _, filenames in os.walk(destination_path):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# It is noted in the Kaggle submission that one can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# It's also noted onc can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-03T16:11:07.180241Z",
          "iopub.execute_input": "2024-10-03T16:11:07.181016Z",
          "iopub.status.idle": "2024-10-03T16:11:07.569854Z",
          "shell.execute_reply.started": "2024-10-03T16:11:07.180969Z",
          "shell.execute_reply": "2024-10-03T16:11:07.568855Z"
        },
        "trusted": true,
        "id": "V3fyPCb7HA7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68ad35e-56a5-4aad-b9b9-f56405b9ebc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/KaggleData/200-bird-species-with-11788-images/segmentations.tgz\n",
            "/content/drive/My Drive/KaggleData/200-bird-species-with-11788-images/CUB_200_2011.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "target_path = '/content/drive/My Drive/KaggleData/200-bird-species-with-11788-images'\n",
        "\n",
        "# Path to the .tgz file\n",
        "tgz_file_path = \"/content/drive/My Drive/KaggleData/200-bird-species-with-11788-images/CUB_200_2011.tgz\"\n",
        "\n",
        "# Extract the .tgz file\n",
        "with tarfile.open(tgz_file_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path = target_path)  # Specify the destination folder\n",
        "tgz_file_path = \"/content/drive/My Drive/KaggleData/200-bird-species-with-11788-images/segmentations.tgz\"\n",
        "\n",
        "# Extract the .tgz file\n",
        "with tarfile.open(tgz_file_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path= target_path)  # Specify the destination folder\n",
        "\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-03T16:11:07.571697Z",
          "iopub.execute_input": "2024-10-03T16:11:07.572112Z",
          "iopub.status.idle": "2024-10-03T16:11:26.435675Z",
          "shell.execute_reply.started": "2024-10-03T16:11:07.572076Z",
          "shell.execute_reply": "2024-10-03T16:11:26.43475Z"
        },
        "trusted": true,
        "id": "uK9YHoEbHA7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58087b8b-7b35-4dcd-c8e7-876fa9015d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3276341342.py:8: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=\"./\")  # Specify the destination folder\n",
            "/tmp/ipython-input-3276341342.py:13: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=\"./\")  # Specify the destination folder\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os # we already did this\n",
        "\n",
        "# Path to the parent directory\n",
        "parent_directory = '/kaggle/working/CUB_200_2011/images'\n",
        "\n",
        "# List of directories inside the parent directory\n",
        "directories = [d for d in os.listdir(parent_directory) if os.path.isdir(os.path.join(parent_directory, d))]\n",
        "\n",
        "directories = sorted(directories)\n",
        "print(directories)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-03T16:11:27.231724Z",
          "iopub.execute_input": "2024-10-03T16:11:27.232551Z",
          "iopub.status.idle": "2024-10-03T16:11:27.240508Z",
          "shell.execute_reply.started": "2024-10-03T16:11:27.232506Z",
          "shell.execute_reply": "2024-10-03T16:11:27.239468Z"
        },
        "trusted": true,
        "id": "EDsfOmuwHA7n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}